{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed638ef7",
   "metadata": {},
   "source": [
    "# Neural Networks with TensorFlow / Keras\n",
    "\n",
    "This notebook (auto-generated) loads the dataset included in the uploaded archive, preprocesses it, builds a neural network with Keras, trains it, shows accuracy & loss curves, and performs a simple hyperparameter tuning sweep (learning rate and batch size).\n",
    "\n",
    "If the archive contains multiple CSV files the largest CSV (by file size) is chosen automatically. The notebook detects whether the problem is classification or regression based on the target column and data types.\n",
    "\n",
    "Sections:\n",
    "1. Setup and dataset loading\n",
    "2. Preprocessing\n",
    "3. Model building\n",
    "4. Training and evaluation\n",
    "5. Simple hyperparameter tuning\n",
    "\n",
    "You can run this notebook in Google Colab, local Jupyter, or any environment with TensorFlow installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detected CSV files (auto-generated)\n",
    "import os\n",
    "csv_files = []\n",
    "for p in csv_files:\n",
    "    print(p, \" - \", os.path.getsize(p), \"bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec69958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: picks the largest CSV file automatically.\n",
    "import pandas as pd, numpy as np, os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "csv_files = []\n",
    "\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(\"No CSV files found in the uploaded archive. If your dataset is not CSV, please modify the notebook accordingly.\")\n",
    "\n",
    "# choose the largest CSV by filesize\n",
    "csv_path = max(csv_files, key=lambda p: os.path.getsize(p))\n",
    "print(\"Using CSV:\", csv_path)\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697504ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing:\n",
    "# - Drop columns with all NaNs\n",
    "# - If there is a 'target' column use it. Else use last column as target.\n",
    "# - Encode categorical targets for classification.\n",
    "# - Split into train/validation/test.\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# df is expected to exist from previous cell\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Heuristic for target\n",
    "target_col = None\n",
    "if 'target' in df.columns:\n",
    "    target_col = 'target'\n",
    "else:\n",
    "    target_col = df.columns[-1]\n",
    "\n",
    "print(\"Using target column:\", target_col)\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# If target looks like string/object -> classification\n",
    "is_classification = y.dtype == object or y.dtype.name == 'category' or y.nunique() < 20\n",
    "\n",
    "print(\"Detected task type:\", \"classification\" if is_classification else \"regression\")\n",
    "if is_classification:\n",
    "    # encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y.astype(str))\n",
    "    class_names = list(le.classes_)\n",
    "    num_classes = len(class_names)\n",
    "    print(\"Classes ({}):\".format(num_classes), class_names)\n",
    "    y = y_enc\n",
    "else:\n",
    "    num_classes = 1\n",
    "\n",
    "# Basic numeric imputation for simplicity\n",
    "X = X.copy()\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == object:\n",
    "        # try to convert to numeric if possible\n",
    "        try:\n",
    "            X[c] = pd.to_numeric(X[c])\n",
    "        except:\n",
    "            X[c] = X[c].astype('category').cat.codes\n",
    "    X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "# Train/val/test split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y if is_classification else None)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1765, random_state=42, stratify=y_trainval if is_classification else None)\n",
    "# that second split makes overall ~15% test, ~15% val, ~70% train\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shapes:\", X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple dense neural network with Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "def make_model(hidden_units=[64,32], dropout=0.2, learning_rate=1e-3, num_classes=1, is_classification=True):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    for u in hidden_units:\n",
    "        model.add(layers.Dense(u, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout))\n",
    "    if is_classification:\n",
    "        if num_classes == 2:\n",
    "            model.add(layers.Dense(1, activation='sigmoid'))\n",
    "            loss = 'binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "        else:\n",
    "            model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "            loss = 'sparse_categorical_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "    else:\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        loss = 'mse'\n",
    "        metrics = ['mse']\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# quick sanity build\n",
    "model = make_model(is_classification=True, num_classes=1)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model and plotting accuracy/loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "early = callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=60,\n",
    "    batch_size=32,\n",
    "    callbacks=[early],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "# accuracy may not be in history if regression\n",
    "if 'accuracy' in history.history:\n",
    "    plt.plot(history.history['accuracy'], label='train_acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.plot(history.history.get('mse', []), label='train_mse')\n",
    "    plt.plot(history.history.get('val_mse', []), label='val_mse')\n",
    "    plt.title('MSE')\n",
    "    plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "eval_res = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Test evaluation:\", eval_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple hyperparameter sweep for learning rate and batch size\n",
    "import numpy as np\n",
    "results = []\n",
    "lrs = [1e-3, 5e-4, 1e-4]\n",
    "batches = [16, 32, 64]\n",
    "\n",
    "for lr in lrs:\n",
    "    for bs in batches:\n",
    "        print(\"Training with lr=\", lr, \"batch=\", bs)\n",
    "        m = make_model(hidden_units=[128,64], dropout=0.3, learning_rate=lr, num_classes=1, is_classification=True)\n",
    "        h = m.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), epochs=30, batch_size=bs, verbose=0,\n",
    "                  callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)])\n",
    "        # pick metric\n",
    "        if 'val_accuracy' in h.history:\n",
    "            score = max(h.history['val_accuracy'])\n",
    "            metric_name = 'val_accuracy'\n",
    "        else:\n",
    "            score = -min(h.history['val_loss'])  # negative to prefer smaller\n",
    "            metric_name = 'val_loss (neg)'\n",
    "        results.append({'lr': lr, 'batch': bs, 'metric_name': metric_name, 'score': score})\n",
    "        print(\" ->\", metric_name, score)\n",
    "\n",
    "import pandas as pd\n",
    "res_df = pd.DataFrame(results)\n",
    "display(res_df.sort_values('score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and scaler for later use\n",
    "model.save(\"/mnt/data/final_keras_model\")\n",
    "import joblib\n",
    "joblib.dump(scaler, \"/mnt/data/scaler.save\")\n",
    "print(\"Saved model to /mnt/data/final_keras_model and scaler to /mnt/data/scaler.save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359dc73d",
   "metadata": {},
   "source": [
    "## Next steps / Tips\n",
    "\n",
    "- If your dataset is images or in a different format, modify the loading/preprocessing cells accordingly.\n",
    "- For better hyperparameter tuning, use Keras Tuner or scikit-learn's GridSearchCV (wrapping Keras model).\n",
    "- Consider feature engineering, regularization, and more layers for complex datasets.\n",
    "\n",
    "The notebook file has been saved alongside this script. Download it using the link provided below."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
